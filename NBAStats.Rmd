---
title: "GroupProject2"
author: "Miguel Bonilla, Jordan Eaddy, Milan Patel"
date: "7/21/2022"
always_allow_html: true
output:
  md_document:
    variant: markdown_github
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r libraries}
library(tidyverse)
library(GGally)
library(MASS)
library(caret)
library(glmnet)
library(kableExtra)
library(DescTools)
library(epitools)
library(lattice)
library(reshape2)
library(ggthemes)
library(cowplot)
library(ROCR)
library(blorr)
library(randomForest)
library(heplots)
```

## load data
will clean and remove opposing team stats, since they're included as the primary on the mirror matchup (i.e. Atl-Tor, vs Tor-Atl)
also removing date, since Game is a better metric of when a game takes place in relation to the point of the season

### Variable Descriptions
* PTS - Team Points Scored
* FG - Field Goals Made
* FGA - Field Goals Attempted
* FG% - Field Goal Percentage
* 3P - Three Points Made
* 3PA - Three Points Attempted
* 3P% - Three Point Percentage
* FT - Free Throws Made
* FTA - Free Throws Attempted
* FT% - Free Throw Percentage
* ORB - Offensive Rebounds
* TRB - Total Rebounds
* AST - Assists
* STL - Steals
* BLK - Blocks
* TOV - Turnovers
* PF - Fouls

```{r wrangling and corr mat}
rawdata <- read.csv("https://raw.githubusercontent.com/boneeyah/GroupProject2/main/DataFile/nba.games.stats.csv")
##change variable names
rawdata <- rename(rawdata, c("PTS" = TeamPoints, "FG"=FieldGoals,"FGA"=FieldGoalsAttempted, "FG%"=FieldGoals., "3PA"=X3PointShotsAttempted, "3P" = X3PointShots, "3P%"=X3PointShots.,"FT"=FreeThrows, "FTA"=FreeThrowsAttempted, "FT%"=FreeThrows.,"ORB"=OffRebounds,"TRB"=TotalRebounds, "AST"=Assists, "STL"=Steals,"BLK"=Blocks,"TOV"=Turnovers, "PF"=TotalFouls, "Opp" = Opponent))
#set categorical variables to factors
rawdata$WINorLOSS <- as.factor(rawdata$WINorLOSS)
rawdata$Home <- as.factor(rawdata$Home)
rawdata$Team <- as.factor(rawdata$Team)
rawdata$Opp <- as.factor(rawdata$Opp)

cleandata <- rawdata[,c(7,3,5,8,10:25)] #rearrange to leave W/L first

str(cleandata) #check variable type

cleandata %>% count(WINorLOSS) %>% ggplot(aes(x=WINorLOSS, y= n, fill = WINorLOSS))+geom_bar(stat = "identity") + scale_fill_viridis_d()+theme_cowplot()
#data is perfectly balanced, becasue for each win on the data set there is an opposing team who lost and vice-versa

sapply(cleandata, function(x) sum(is.na(x))) #no NAs present

ggpairs(cleandata, columns = 2:10, aes(color = WINorLOSS), upper = list(continuous = wrap("cor", size=3, color="black")))+theme_cowplot()+scale_fill_viridis_d()+scale_color_viridis_d()+theme(text = element_text(size = 10),axis.text =element_blank())
ggpairs(cleandata, columns = 11:20, aes(color = WINorLOSS), upper = list(continuous = wrap("cor", size=3, color = "black"))) + scale_color_viridis_d()+scale_fill_viridis_d()+theme_cowplot()+theme(text = element_text(size = 10),axis.text =element_blank())

```
There is a separation between home and away
also for team points, field goals, field goal %, 3 point shots and 3 point shot %
to a lesser extent, total rebounds, assists, and turnovers
There is an outlier for offensive rebounds and total rebounds. It comes from a game between Milwaukee and Brooklyn which went to triple over time and had 81 total rebounds for the Bucks, who lost the game.
We validated the number of rebounds, and since there is no error, we decided not to remove the observation from our dataset
```{r var plots}
cleandata %>% ggplot(aes(x=Home, fill=WINorLOSS))+geom_bar(position = "fill")+scale_fill_viridis_d()+labs(y="WINorLOSS")+theme_cowplot()
cleandata %>% ggplot(aes(y=PTS, x=WINorLOSS, fill=WINorLOSS))+geom_boxplot()+scale_fill_viridis_d()+theme_cowplot()
cleandata %>% ggplot(aes(y=`FG%`,x=WINorLOSS, fill=WINorLOSS))+geom_boxplot()+scale_fill_viridis_d()+theme_cowplot()
cleandata %>% ggplot(aes(y=FG, x=WINorLOSS, fill=WINorLOSS))+geom_boxplot()+scale_fill_viridis_d()+theme_cowplot()
cleandata %>% ggplot(aes(y=`3PA`, x= WINorLOSS, fill=WINorLOSS))+geom_boxplot()+scale_fill_viridis_d()+theme_cowplot()
cleandata %>% ggplot(aes(y=`3P%`,x=WINorLOSS, fill=WINorLOSS))+geom_boxplot()+scale_fill_viridis_d()+theme_cowplot()
cleandata %>% ggplot(aes(y=TRB, x=WINorLOSS, fill=WINorLOSS))+geom_boxplot()+scale_fill_viridis_d()+theme_cowplot()
cleandata %>% ggplot(aes(y=AST, x=WINorLOSS, fill=WINorLOSS))+geom_boxplot()+scale_fill_viridis_d()+theme_cowplot()
```
##correlation plot
build a heatmap to check for correlation between explanatory variables
```{r heatmap}

cleandata.corr <- round(cor(cleandata[,c(2,4:20)]),2)
cleandata.corr <- melt(cleandata.corr)

cleandata.corr %>% ggplot(aes(x=Var1, y=Var2, fill=value))+geom_tile()+scale_fill_viridis_c()+theme_cowplot()+theme(axis.title.x = element_blank(),axis.title.y = element_blank(), axis.text = element_text(size = 10))
```
Not surprisingly, points and field goals, free throw attempts and free throws seem to have a strong correlation
```{r split and PCA}
#set seed
set.seed(1776)
##80-20 split of data
index <- sample(nrow(cleandata), round(.8*nrow(cleandata)))
train <- cleandata[index,]
test <- cleandata[-index,]

#new set with only numerical variables
#removed colinear vars fg, 3p, ft
numdata <- rawdata[,c(7,3,8,11,12,14,15,17:25)]
train.num <- numdata[index,]
test.num <- numdata[-index,]

#run PCA
PCA <-  prcomp(train.num[,-1], scale. = TRUE)
PCA
summary(PCA)
```

```{r fit a simple model}

simple.mod <- glm(WINorLOSS~1, family = "binomial", data=train)#intercept only simple model
coef(simple.mod)
summary(simple.mod)

simple.pred <- predict(simple.mod, newdata = test, type = "response")
simple.auc <- performance(prediction(simple.pred, test$WINorLOSS), "auc")
simple.auc <- simple.auc@y.values[[1]]
simple.pred <- ifelse(simple.pred > .5, "W", "L")
simple.pred <- factor(simple.pred, levels = c("W","L"))
test$WINorLOSS <- factor(test$WINorLOSS, levels = c("W", "L"))


cm.simple <- confusionMatrix(data =simple.pred, reference = test$WINorLOSS)
cm.simple
```
now adding feature selection to see if the model can improve with the table in it's present format
```{r fwd bkw and step}
empty.mod <- glm(WINorLOSS~ 1,family = "binomial", data = train) #as a starting point for forward and stepwise selection
full.mod <- glm(WINorLOSS~.,family = "binomial", data=train) #starting point for backward selection
step.mod <- empty.mod %>% stepAIC(direction = "both", scope = list(lower = empty.mod, upper=~Game + Home + PTS + FG + FGA + `FG%` + `3P` + `3PA` + `3P%` + FT + FTA + `FT%` + ORB + TRB + AST + STL + BLK + TOV + PF),trace = FALSE)
summary(step.mod)

step.pred <- predict(step.mod, newdata = test, type = "response")
step.auc <- performance(prediction(step.pred, test$WINorLOSS),"auc")
step.auc <- step.auc@y.values[[1]]
step.pred <- ifelse(step.pred>.5, "W", "L")
step.pred <- factor(step.pred, levels = c("W","L"))

cm.step <- confusionMatrix(data = step.pred, reference = test$WINorLOSS)

fwd.mod <- empty.mod %>% stepAIC(direction = "forward", scope = list(lower = empty.mod, upper=~Game + Home + PTS + FG + FGA + `FG%` + `3P` + `3PA` + `3P%` + FT + FTA + `FT%` + ORB + TRB + AST + STL + BLK + TOV + PF),trace = FALSE)
summary(fwd.mod)
fwd.pred <- predict(fwd.mod, newdata = test, type = "response")
fwd.auc <- performance(prediction(fwd.pred, test$WINorLOSS), "auc")
fwd.auc <- fwd.auc@y.values[[1]]
fwd.pred <- ifelse(fwd.pred>.5, "W", "L")
fwd.pred <- factor(fwd.pred, levels = c("W","L"))

cm.fwd <- confusionMatrix(data = fwd.pred, reference = test$WINorLOSS)

bkw.mod <- full.mod %>% stepAIC(direction = "backward", trace = FALSE)
summary(bkw.mod)
bkw.pred <- predict(bkw.mod, newdata = test, type = "response")
bkw.auc <- performance(prediction(bkw.pred, test$WINorLOSS), "auc")
bkw.auc <- bkw.auc@y.values[[1]]
bkw.pred <- ifelse(bkw.pred>.5, "W", "L")
bkw.pred <- factor(bkw.pred, levels = c("W","L"))

cm.bkw <- confusionMatrix(data = bkw.pred, reference = test$WINorLOSS)
```
The variable Game, probably does not make sense in the context of our dataset without an interaction. Since for any given game there are exactly the same number of wins and losses (each match has one winner and one loser).
We will therefore fit a new model without Game for our no interaction model, keeping in mind that there might be potential interactions between Game and other variables which might make for a better more complex model in Objective 2.
From correlation scatterplot and heatmap there is some evidence of potential colinearity between TRB and ORB. Coupled with context knowledge that Offensive rebounds are one of two values that go into total rebounds, we decided to fit the model without ORB.

Fitting a custom model without ORB and without Game variable

```{r custom}
custom.mod <- glm(WINorLOSS~ Home + PTS + FGA + `3PA` + `3P%` + FTA + TRB + AST + STL + BLK + TOV + PF, family = "binomial", data = train)
summary(custom.mod)

custom.pred <- predict(custom.mod, newdata = test, type = "response")
custom.auc <- performance(prediction(custom.pred, test$WINorLOSS), "auc")
custom.auc <- custom.auc@y.values[[1]]
custom.pred <- ifelse(custom.pred>.5, "W", "L")
custom.pred <- factor(custom.pred, levels = c("W","L"))

cm.custom <- confusionMatrix(custom.pred, test$WINorLOSS)
cm.custom
```
Finally, we will now use LASSO for selection to compare to the simple 
```{r lasso}
train.x <- model.matrix(WINorLOSS~.-1, data = train) #-1 removes intercept column
train.y <- train[,1]

cvfit <- cv.glmnet(train.x, train.y, family = "binomial", type.measure = "class")

coef(cvfit)

lasso.mod <- glm(WINorLOSS~ Game + Home + PTS + FGA + `FG%` + `3P` + `3P%` + `FT%` + ORB + TRB + AST + STL + BLK + TOV + PF, family = "binomial", data = train)

lasso.pred <- predict(lasso.mod, newdata = test, type = "response")
lasso.auc <- performance(prediction(lasso.pred, test$WINorLOSS), "auc")
lasso.auc <- lasso.auc@y.values[[1]]
lasso.pred <- ifelse(lasso.pred>.5, "W", "L")
lasso.pred <- factor(lasso.pred, levels = c("W","L"))

cm.lasso <- confusionMatrix(data = lasso.pred, reference = test$WINorLOSS)
cm.lasso
```
Create a table with the results from all the confusion Matrix models
```{r perf met table}
cm.df <- data.frame("Model" = c("Simple", "Forward", "Stepwise", "Backward", "Custom", "LASSO"),
           "Accuracy"= c(cm.simple$overall[1],cm.fwd$overall[1], cm.step$overall[1],cm.bkw$overall[1], cm.custom$overall[1],cm.lasso$overall[1]),
           "Sensitivity"=c(cm.simple$byClass[1],cm.fwd$byClass[1], cm.step$byClass[1], cm.bkw$byClass[1], cm.custom$byClass[1], cm.lasso$byClass[1]),
           "Specificty" = c(cm.simple$byClass[2],cm.fwd$byClass[2], cm.step$byClass[2],cm.bkw$byClass[2], cm.custom$byClass[2], cm.lasso$byClass[2]),
           "AUC" = c(simple.auc, fwd.auc, step.auc, bkw.auc, custom.auc, lasso.auc))
cm.df <- kable(cm.df, format = "html") %>% kable_styling(latex_options = c("striped", "scale_down"), full_width = FALSE) %>% 
  row_spec(row = 0, italic = T, background = "#21918c", color = "white") %>% 
  column_spec(1:2, width = "0.5in")
cm.df
```

ROC for objective 1, with lower complexity data
```{r roc plot}


custom.pred.roc <- prediction(predict(custom.mod, newdata = test, type = "response"), test$WINorLOSS)
custom.roc <- performance(custom.pred.roc, "tpr", "fpr")

lasso.pred.roc <- prediction(predict(lasso.mod, newdata = test, type = "response"), test$WINorLOSS)
lasso.roc <- performance(lasso.pred.roc, "tpr", "fpr")
step.pred.roc <- prediction(predict(step.mod, newdata = test, type = "response"), test$WINorLOSS)
step.roc <- performance(step.pred.roc, "tpr", "fpr")
bkw.pred.roc <- prediction(predict(bkw.mod, newdata = test, type = "response"), test$WINorLOSS)
bkw.roc <- performance(bkw.pred.roc, "tpr", "fpr")
simple.pred.roc <- prediction(predict(simple.mod, newdata = test, type = "response"), test$WINorLOSS)
simple.roc <- performance(simple.pred.roc, "tpr", "fpr")

df.roc <- rbind(data.frame(x.vals=simple.roc@x.values[[1]], y.vals=simple.roc@y.values[[1]],model = "simple"),
                data.frame(x.vals=bkw.roc@x.values[[1]], y.vals=bkw.roc@y.values[[1]], model="bkw"),
                data.frame(x.vals=step.roc@x.values[[1]], y.vals=step.roc@y.values[[1]], model="step"),
                data.frame(x.vals=lasso.roc@x.values[[1]], y.vals=lasso.roc@y.values[[1]], model="lasso"),
                data.frame(x.vals=custom.roc@x.values[[1]], y.vals=custom.roc@y.values[[1]], model="custom"))
df.roc$model <- factor(df.roc$model, levels = c("simple", "bkw", "step", "lasso", "custom"))
df.roc %>% ggplot(aes(x=x.vals,y=y.vals, color = model))+geom_line(size=.55)+theme_cowplot()+labs(x="False positive rate", y="True positive rate")+scale_color_viridis_d(direction = -1)
```

All the models, with the exception of the Simple model, perform reasonably well and similar to each other. Forward and Stepwise have the exact same performance, in this case they both selected the same model, which is not uncommon given that Stepwise is a modified version of forward selection (it means that the stepwise selection did not remove any variables once they were added, so it behaved the same as forward selection).

Our custom model performs marginally better, with 1% higher Specificity (properly predicting true negatives), with slightly higher accuracy than the rest of the models
```{r full model}
#fit model with all data
custom.mod.full <- glm(WINorLOSS~ Home + PTS + FGA + `3PA` + `3P%` + FTA + TRB + AST + STL + BLK + TOV + PF, family = "binomial", data = cleandata)
summary(custom.mod.full)

custom.pred.full <- custom.mod.full$fitted.values
custom.pred.full2 <- ifelse(custom.pred.full>.5, "W", "L")
custom.pred.full2 <- factor(custom.pred.full2, levels = c("W","L"))

cm.custom.full <- confusionMatrix(custom.pred.full2, factor(cleandata$WINorLOSS, levels = c("W","L")))
cm.custom.full

```
Calculate leverage and influence plots to check assumptions are met
```{r assumptions}
## residuals for high leverage outliers
blr_plot_leverage(custom.mod.full) #identifying any particularly high leverage observations
blr_plot_leverage_fitted(custom.mod.full) # high leverage fitted values
blr_plot_pearson_residual(custom.mod.full) #pearson residuals for each observation
plot(custom.mod.full, col = "blue") # plot 4 high leverage cook's D values

# correlation for colinearity
custom.mod.corr <- round(cor(numdata[,c(3,4,6,7,8,11:16)]),2)
custom.mod.corr %>% kable(format = "html") %>% kable_styling(latex_options = c("striped", "scale_down"), full_width = FALSE) %>% 
  row_spec(row = 0, italic = T, background = "#21918c", color = "white") %>% 
  column_spec(column = 1, italic = T, background = "#21918c", color ="white") #correlation matrix, difficult to see
custom.mod.corr <- melt(custom.mod.corr)
custom.mod.corr %>% ggplot(aes(x=Var1, y = Var2, fill = value))+geom_tile()+scale_fill_viridis_c()+theme_cowplot()+theme(axis.title.x = element_blank(),axis.title.y = element_blank(), axis.text = element_text(size = 10))

#linearity check
cleandata %>% mutate("Log Odds" = custom.mod.full$linear.predictors) %>% dplyr::select(c(21,4,6,9,10,12,15:20)) %>% 
  pivot_longer(cols = 2:12, names_to = "statistic") %>% ggplot(aes(x=value,y=`Log Odds`))+geom_smooth(fill = "#440154", alpha = .3, method = "loess", size = 1, span = 1.25)+facet_wrap(vars(statistic), scales = "free")+theme_cowplot()+theme(axis.text = element_blank())
```

create table with interpretable values on odds ratio scale

```{r interpretation table}
## p hat probably doesn't make sense in this context, will remove
data.frame("coefficient" = coef(custom.mod.full),
           "Odds Ratio" = exp(coef(custom.mod.full)),
           exp(confint.default(custom.mod.full, level = .95))) %>% 
  kable(format = "html", col.names = c("Coefficient", "Odds Ratio", "2.5%", "97.5%")) %>% 
  kable_styling(latex_options = c("striped", "scale_down"), full_width = FALSE) %>% 
  row_spec(row = 0, italic = T, background = "#21918c", color = "white") %>% 
  column_spec(1:2, width = "0.5in")

```
## Objective 2
Creating a complex model introducing interactions
Added team and Opp Categorical variables, largely increases complexity since they're categorical variables
with 30 levels
```{r complex model}
complexdata <- rawdata[,c(7,2,3,5,6,8,10:25)]
complexdata <- complexdata %>%  mutate(PTSsq = PTS^2, DIV = 2*((TRB + AST + 4*STL + 2*BLK)-(4*TOV + 2*PF)))
train.complex <- complexdata[index,]
test.complex <- complexdata[-index,]

complex.mod <- glm(formula = WINorLOSS ~ Team + Home + Opp + PTS + FGA + 
                     `3P%` + `3PA` + FTA + `FT%` + TRB + AST + 
                     STL + BLK + TOV + PF + TOV:Opp, family = "binomial", data = train.complex)
complex.pred <- predict(complex.mod, test.complex, type = "response")
complex.auc <- performance(prediction(complex.pred, test.complex$WINorLOSS),"auc")
complex.auc <- complex.auc@y.values[[1]]
complex.pred <- ifelse(complex.pred>.5, "W", "L")
complex.pred <- factor(complex.pred, levels = c("W", "L"))
test.complex$WINorLOSS <- factor(test.complex$WINorLOSS, levels = c("W", "L"))

cm.complex <- confusionMatrix(complex.pred, test.complex$WINorLOSS)
cm.complex
```
LASSO fit
```{r lasso 2}
train.complex.x <- model.matrix(WINorLOSS~.-1, data = train.complex) #-1 removes intercept column
train.complex.y <- train.complex[,1]

cvfit2 <- cv.glmnet(train.complex.x, train.complex.y, family = "binomial", type.measure = "class")
plot(cvfit2)
coef(cvfit2, s= "lambda.min")

paste("optimal LASSO penalty value",sprintf("%.10f",cvfit2$lambda.min)) # this is the optimal LASSO penalty value

lasso.mod2 <- glmnet(train.complex.x, train.complex.y, family = "binomial", lambda = cvfit2$lambda.min)

coef(lasso.mod2)

test.complex.x <- model.matrix(WINorLOSS~.-1, data = test.complex)
lasso.pred2 <- predict(lasso.mod2, newx = test.complex.x, type = "response")
lasso.auc2 <- performance(prediction(lasso.pred2,test.complex$WINorLOSS),"auc")
lasso.auc2 <- lasso.auc2@y.values[[1]]
lasso.pred2 <- ifelse(lasso.pred2>.5, "W", "L")
lasso.pred2 <- factor(lasso.pred2, levels = c("W","L"))

cm.lasso2 <- confusionMatrix(data = lasso.pred2, reference = test.complex$WINorLOSS)
cm.lasso2
```

Create QDA LDA models
```{r LDA QDA}
#quick assumption check of equal covariances by using ellispes plot
covEllipses(train.num[,2:16], train.num$WINorLOSS, fill = TRUE, pooled = FALSE, variables = 1:15, col = c("#277f8e","#440154"), fill.alpha = .05)
lda.mod <- lda(WINorLOSS~., data=train.num, prior = c(.5,.5))
lda.pred <-  predict(lda.mod, test.num)
lda.auc <- performance(prediction(lda.pred$posterior[,2],test.num$WINorLOSS),"auc")
lda.auc <- lda.auc@y.values[[1]]

cm.lda <- confusionMatrix(lda.pred$class, test.num$WINorLOSS)
cm.lda

qda.mod <- qda(WINorLOSS~., data=train.num, prior = c(.5,.5))
qda.pred <- predict(qda.mod, test.num)
qda.auc <- performance(prediction(qda.pred$posterior[,2],test.num$WINorLOSS),"auc")
qda.auc <- qda.auc@y.values[[1]]

cm.qda <- confusionMatrix(qda.pred$class, test.num$WINorLOSS)
cm.qda
```

## random forests classification
finding the tuning parameter mtry takes the longest from running this code
it can be skipped to run this much faster
```{r}

train.expl <- train.complex[,2:24]
train.resp <- train.complex[,1]

train(train.expl, train.resp, method = "rf") # this takes 10+ minutes to run, # returns ideal mtry=12

```
```{r}
rfdata <- complexdata %>% rename("FGp" = "FG%", "3P","ThP" = "3P", "ThPA" = "3PA", "ThPp" = "3P%", "FTp" = "FT%")
train.rf <- rfdata[index,]
test.rf <- rfdata[-index,]
rf.mod <- randomForest(WINorLOSS~., data = train.rf, mtry = 12, ntree =1000, cutoff = c(.45,.55))
rf.pred <- predict(rf.mod, test.rf)
rf.auc <- performance(prediction(predict(rf.mod, test.rf, type = "prob")[,2],test.rf$WINorLOSS),"auc", x.measure = "cutoff")
rf.auc <- rf.auc@y.values[[1]]
rf.pred <- factor(rf.pred, levels = c("W", "L"))
test.rf$WINorLOSS <- factor(test.rf$WINorLOSS, levels = c("W","L"))

cm.rf <- confusionMatrix(rf.pred, test.rf$WINorLOSS)
cm.rf
```

Update table with Obj 2 models
```{r new table}
cm.df <- data.frame("Model" = c("Simple", "Forward", "Stepwise", "Backward", "Custom", "LASSO", "Complex", "LASSO 2", "LDA", "QDA", "RandomForests"),
           "Accuracy"= c(cm.simple$overall[1],cm.fwd$overall[1], cm.step$overall[1],cm.bkw$overall[1], cm.custom$overall[1],cm.lasso$overall[1],cm.complex$overall[1], cm.lasso2$overall[1], cm.lda$overall[1], cm.qda$overall[1], cm.rf$overall[1]),
           "Sensitivity"=c(cm.simple$byClass[1],cm.fwd$byClass[1], cm.step$byClass[1], cm.bkw$byClass[1], cm.custom$byClass[1], cm.lasso$byClass[1], cm.complex$byClass[1], cm.lasso2$byClass[1], cm.lda$byClass[1], cm.qda$byClass[1], cm.rf$byClass[1]),
           "Specificty" = c(cm.simple$byClass[2],cm.fwd$byClass[2], cm.step$byClass[2],cm.bkw$byClass[2], cm.custom$byClass[2], cm.lasso$byClass[2], cm.complex$byClass[2], cm.lasso2$byClass[2], cm.lda$byClass[2], cm.qda$byClass[2], cm.rf$byClass[2]),
           "AUC" = c(simple.auc, fwd.auc, step.auc, bkw.auc, custom.auc, lasso.auc, complex.auc, lasso.auc2, lda.auc, qda.auc, rf.auc))
cm.df <- kable(cm.df, format = "html") %>% kable_styling(latex_options = c("striped", "scale_down"), full_width = FALSE) %>% 
  row_spec(row = 0, italic = T, background = "#21918c", color = "white") %>% 
  column_spec(1:2, width = "0.5in")
cm.df
```

New ROC with Obj 2 models
Might Add LDA and QDA to this
Need to add Legend
```{r new roc plot}

complex.pred.roc <- prediction(predict(complex.mod, newdata = test.complex, type = "response"), test.complex$WINorLOSS)
complex.roc <- performance(complex.pred.roc, "tpr", "fpr")
lasso2.pred.roc <- prediction(predict(lasso.mod2, newx = test.complex.x, type = "response"), test.complex$WINorLOSS)
lasso2.roc <- performance(lasso2.pred.roc, "tpr", "fpr")
lda.pred.roc <- prediction(lda.pred$posterior[,2],test.num$WINorLOSS)
lda.roc <- performance(lda.pred.roc, "tpr", "fpr")
qda.pred.roc <- prediction(qda.pred$posterior[,2], test.num$WINorLOSS)
qda.roc <- performance(qda.pred.roc, "tpr", "fpr")
rf.pred.roc <- prediction(predict(rf.mod, test.rf, type = "prob")[,2],test.rf$WINorLOSS)
rf.roc <- performance(rf.pred.roc, "tpr", "fpr")

df.roc <- rbind(data.frame(x.vals=lasso2.roc@x.values[[1]], y.vals=lasso2.roc@y.values[[1]],model="lasso2"),
                data.frame(x.vals=lda.roc@x.values[[1]], y.vals=lda.roc@y.values[[1]], model="lda"),
                data.frame(x.vals=qda.roc@x.values[[1]], y.vals=qda.roc@y.values[[1]], model="qda"),
                data.frame(x.vals=rf.roc@x.values[[1]], y.vals=rf.roc@y.values[[1]], model = "rf"),
                data.frame(x.vals=complex.roc@x.values[[1]], y.vals=complex.roc@y.values[[1]], model="complex"),
                df.roc)
df.roc$model <- factor(df.roc$model, levels = c("simple", "bkw", "step", "lasso", "custom", "rf", "lda", "qda", "lasso2", "complex"))

df.roc %>% filter(model %in% c("custom", "rf", "lda", "qda", "lasso2", "complex")) %>% 
  ggplot(aes(x=x.vals, y=y.vals, color = model))+geom_line()+theme_cowplot()+scale_color_viridis_d(direction = -1)+labs(x="False positive rate", y="True positive rate")

plot(custom.roc, col="#fde725", lwd=1)
plot(lasso2.roc, lwd = 1, add = TRUE, col = "#3b528b")
plot(lda.roc, lwd = 1, add = TRUE, col = "#21918c")
plot(qda.roc, lwd = 1, add = TRUE, col = "#7ad151")
plot(rf.roc, lwd = 1, add = TRUE, col = "#414487")
plot(complex.roc, col = "#440154", lwd = 1, add = TRUE)
legend(x = .75, y = .50, legend = c("simple", "backwards", "stepwise", "lasso", "custom","complex"), col = c("#fde725", "#3b528b", "#21918c", "#7ad151", "#414487", "440154"), lty =1)

```
```{r complex over full data}
complex.mod.full <- glm(formula = WINorLOSS ~ Team + Home + Opp + PTS + FGA + 
                     `3P%` + `3PA` + FTA + `FT%` + TRB + AST + 
                     STL + BLK + TOV + PF + TOV:Opp, family = "binomial", data = complexdata)
complex.pred.full <- predict(complex.mod.full, complexdata, type = "response")
complex.auc.full <- performance(prediction(complex.pred.full, complexdata$WINorLOSS),"auc")
complex.auc.full <- complex.auc.full@y.values[[1]]
complex.pred.full <- ifelse(complex.pred>.5, "W", "L")
complex.pred.full <- factor(complex.pred.full, levels = c("W", "L"))
complexdata$WINorLOSS <- factor(complexdata$WINorLOSS, levels = c("W", "L"))

cm.complex.full <- confusionMatrix(factor(ifelse(complex.mod.full$fitted.values>.5,"W","L"),levels = c("W","L")),complexdata$WINorLOSS)
cm.complex.full
paste("AUC",complex.auc.full)
```

